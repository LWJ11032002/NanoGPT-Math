{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "124a869a",
      "metadata": {
        "id": "124a869a"
      },
      "source": [
        "### Step 1: Install necesscary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3b82f8f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b82f8f1",
        "outputId": "a37c8989-740a-4457-c470-d75ef00031d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.41.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "454083b3-4a7c-4bc7-8b60-7bfec77c8bdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "454083b3-4a7c-4bc7-8b60-7bfec77c8bdd",
        "outputId": "a17fc9ea-872e-400b-d173-9966cc503042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pOa2pMJcIUD",
        "outputId": "3f126b81-e15f-4aee-966e-228d8dd8aef3"
      },
      "id": "1pOa2pMJcIUD",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Ass1 github/dpo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFWX_UNMcOLS",
        "outputId": "107b9cda-3fa1-4812-9784-126f19f03353"
      },
      "id": "WFWX_UNMcOLS",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Ass1 github/dpo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2d9de0",
      "metadata": {
        "id": "6c2d9de0"
      },
      "source": [
        "### Step 2: Package imports and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "876dd92d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "876dd92d",
        "outputId": "58f7f372-3606-4063-bd75-72230e487c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(\"..\"))\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import pickle\n",
        "from model import GPT, GPTConfig\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "# Configuration\n",
        "beta = 0.5\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"cuda\")\n",
        "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "base_lr = 0.0002\n",
        "epochs = 30\n",
        "batch_size = 128\n",
        "max_length =80\n",
        "num_samples = 1\n",
        "max_new_tokens = 200\n",
        "temperature = 0.8\n",
        "top_k = 200\n",
        "# tokenizer\n",
        "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__, torch.version.cuda, torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPg6nZAIeGNZ",
        "outputId": "8712d5d1-a208-4932-9b10-ce537272d522"
      },
      "id": "MPg6nZAIeGNZ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126 12.6 True\n",
            "NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7d35e6",
      "metadata": {
        "id": "4c7d35e6"
      },
      "source": [
        "### Step 3: Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d03655c3",
      "metadata": {
        "id": "d03655c3"
      },
      "outputs": [],
      "source": [
        "def compute_logprob(input_ids):\n",
        "    inputs = input_ids[:, :-1]\n",
        "    targets = input_ids[:, 1:]\n",
        "    logits, _ = gpt(inputs, full_seq=True)\n",
        "    B, T, V = logits.size()\n",
        "    logits_flat = logits.reshape(-1, V)\n",
        "    targets_flat = targets.reshape(-1)\n",
        "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
        "    loss = loss.reshape(B, T)\n",
        "    attention_mask = (targets != 0).float()\n",
        "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
        "    return -loss\n",
        "\n",
        "def pad_or_truncate(seq, max_length):\n",
        "    return seq[:max_length] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
        "\n",
        "def get_batches(lines, batch_size):\n",
        "    random.shuffle(lines)\n",
        "    #for l in lines:\n",
        "    #    print(l[1])\n",
        "    for i in range(0, len(lines), batch_size):\n",
        "        batch = lines[i:i+batch_size]\n",
        "        if len(batch) < batch_size:\n",
        "            continue\n",
        "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
        "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
        "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
        "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
        "        yield neg_tensor, pos_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9d9eba",
      "metadata": {
        "id": "fc9d9eba"
      },
      "source": [
        "### Step 4: Load the pretrained NanoGPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ceae772a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceae772a",
        "outputId": "eff77ff8-9b59-4294-acb2-f1f69c63458c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(74, 348)\n",
              "    (wpe): Embedding(256, 348)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
              "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
        "gptconf = GPTConfig(**ckpt['model_args'])\n",
        "gpt = GPT(gptconf)\n",
        "state_dict = ckpt['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k in list(state_dict.keys()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "gpt.load_state_dict(state_dict)\n",
        "gpt.to(device).train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0feafc5a",
      "metadata": {
        "id": "0feafc5a"
      },
      "source": [
        "### Step 5: Load Data (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7edf3d44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7edf3d44",
        "outputId": "b28566f8-4de9-40d0-8e1d-754031da6684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "55380\n",
            "{'negative': 'x-21=21, x=? Sorry, I do not know', 'positive': 'x-21=21, x=? The answer is 42 because 21 plus 21 equals 42.'}\n"
          ]
        }
      ],
      "source": [
        "# Load data from ./data/pos_neg_pairs.json\n",
        "with open('/content/drive/MyDrive/Ass1 github/dpo/pos_neg_pairs.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "    lines = data\n",
        "\n",
        "# Check the structure\n",
        "print(type(data))      # should be <class 'list'>\n",
        "print(len(data))       # number of items\n",
        "print(data[1])         # show the first item"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e5f81f",
      "metadata": {
        "id": "c2e5f81f"
      },
      "source": [
        "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "df0c400f",
      "metadata": {
        "id": "df0c400f"
      },
      "outputs": [],
      "source": [
        "def build_optimizer_and_scheduler(\n",
        "    model,\n",
        "    lr=base_lr,\n",
        "    weight_decay=0.05,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-8,\n",
        "    step_size=1500,\n",
        "    gamma=0.95,\n",
        "):\n",
        "    # ---- no weight decay for bias & LayerNorm/BatchNorm ----\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if p.ndim >= 2:\n",
        "            decay.append(p)\n",
        "        else:\n",
        "            no_decay.append(p)\n",
        "\n",
        "    optim_groups = [\n",
        "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
        "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
        "    ]\n",
        "\n",
        "    # AdamW optimizer\n",
        "    optimizer = AdamW(optim_groups, lr=lr, betas=betas, eps=eps)\n",
        "\n",
        "    # Step Decay scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "optimizer, scheduler = build_optimizer_and_scheduler(\n",
        "    gpt,\n",
        "    lr=base_lr,\n",
        "    weight_decay=0.05,\n",
        "    betas=(0.9, 0.95),\n",
        "    eps=1e-8,\n",
        "    step_size=1500,\n",
        "    gamma=0.95,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b66199",
      "metadata": {
        "id": "52b66199"
      },
      "source": [
        "### Step 7: Begin training (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1d4ebeb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d4ebeb4",
        "outputId": "cc64a785-a91a-4cee-cf6e-6e224b7f6f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.69it/s, loss=0.0273, lr=0.0001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.89it/s, loss=0.024, lr=0.0001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.88it/s, loss=0.023, lr=0.0001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.87it/s, loss=0.0202, lr=0.0001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.87it/s, loss=0.0188, lr=0.0001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.87it/s, loss=0.0182, lr=9.5e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.87it/s, loss=0.0177, lr=9.5e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.85it/s, loss=0.0169, lr=9.5e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.87it/s, loss=0.0173, lr=9.5e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.86it/s, loss=0.0167, lr=9.5e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.86it/s, loss=0.0168, lr=9.5e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.86it/s, loss=0.0168, lr=9.02e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.86it/s, loss=0.0168, lr=9.02e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.87it/s, loss=0.0164, lr=9.02e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "865it [00:43, 19.86it/s, loss=0.0166, lr=9.02e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        }
      ],
      "source": [
        "total_steps = len(lines) // batch_size\n",
        "for epoch in range(epochs):\n",
        "    pbar = tqdm(get_batches(lines, batch_size))\n",
        "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
        "        ###########################################################\n",
        "        # Please complete the training code here!\n",
        "        # Examples:\n",
        "        # ...\n",
        "        # neg_logprob\n",
        "        # pos_logprob\n",
        "        # loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
        "        # ...\n",
        "        pos_logprob = compute_logprob(pos_tensor)\n",
        "        neg_logprob = compute_logprob(neg_tensor)\n",
        "\n",
        "        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if (step + 1) % 10 == 0:\n",
        "            pbar.set_postfix(loss=float(loss.item()), lr=scheduler.get_last_lr()[0])\n",
        "\n",
        "        ###########################################################\n",
        "    ckpt_path = f\"./dpo.pt\"\n",
        "    torch.save({\n",
        "        \"model_state_dict\": gpt.state_dict(),\n",
        "        \"model_args\": ckpt['model_args'],\n",
        "    }, ckpt_path)\n",
        "    print(f\"Saved checkpoint to {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b7f2ab",
      "metadata": {
        "id": "48b7f2ab"
      },
      "source": [
        "# Step 8: Begin testing (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "09027262",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09027262",
        "outputId": "0757280e-06ee-427f-ca49-9f92c0b82dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: 17+19=?\n",
            "A: The answer is 36 because 17+19 equals 36.\n",
            "\n",
            "Q: 3*17=?\n",
            "A: The answer is 37 because 3*1 equals 377.\n",
            "\n",
            "Q: 72/4=?\n",
            "A: The answer is 12 because 72 divided by 4 equals 12.\n",
            "\n",
            "Q: 72-x=34,x=?\n",
            "A: The answer is 68 because 72 minus 3 equals 68.\n",
            "\n",
            "Q: x*11=44,x=?\n",
            "A: The answer is 44 because 44 divided by 11 equals 4.\n",
            "\n",
            "Q: 3*17=?\n",
            "A: The answer is 37 because 3*1 equals 377.\n",
            "\n",
            "Q: 72/4=?\n",
            "A: The answer is 14 because 72 divided by 4 equals 14.\n",
            "\n",
            "Q: 72-x=34,x=?\n",
            "A: The answer is 68 because 72 minus 3 equals 68.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model\n",
        "ckpt_path = \"../dpo/dpo.pt\"\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "gpt = GPT(gptconf).to(device)\n",
        "try:\n",
        "    state_dict = checkpoint['model']\n",
        "except:\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "gpt.load_state_dict(state_dict)\n",
        "# Test\n",
        "gpt.eval()\n",
        "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
        "with torch.no_grad():\n",
        "    for prompt in test_set:\n",
        "        prompt_ids = encode(prompt)\n",
        "        ###########################################################\n",
        "        # Please complete the test code here!\n",
        "        # ...\n",
        "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "        # ...\n",
        "        x = torch.tensor([prompt_ids], dtype=torch.long, device=device)  # shape [1, T]\n",
        "\n",
        "        y = gpt.generate(\n",
        "            x,\n",
        "            max_new_tokens=max_new_tokens,      # you defined these above\n",
        "            temperature=temperature,\n",
        "            top_k=top_k\n",
        "        )\n",
        "        out_full = decode(y[0].cpu().flatten().tolist())\n",
        "        generated = out_full[len(prompt):].strip()\n",
        "\n",
        "        print(f\"Q: {prompt}\")\n",
        "        print(f\"A: {generated}\\n\")\n",
        "        ###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1872b7b-30d1-4ba8-83c8-19422e65b66d",
      "metadata": {
        "id": "c1872b7b-30d1-4ba8-83c8-19422e65b66d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (pytorch)",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}